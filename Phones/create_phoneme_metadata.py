#!/usr/bin/env python3
"""
LRS3 Phoneme Metadata Generator

This script converts word-level transcripts to phoneme-level transcripts using G2P (Grapheme-to-Phoneme) 
conversion. It reads the .wrd files generated by the LRS3 preprocessing pipeline and creates corresponding 
.phn files along with a phoneme dictionary.

Usage:
    python create_phoneme_metadata.py --metadata-dir /path/to/lrs3/metadata [options]

The script will:
1. Convert word transcripts (.wrd files) to phoneme transcripts (.phn files)
2. Create a phoneme dictionary (dict.phn.txt) with proper counts for training
3. Generate statistics about the phoneme conversion process

Author: LRS3 Data Preparation Pipeline
"""

import os
import argparse
from pathlib import Path
from g2p_en import G2p
import tqdm
import re
from collections import Counter

def remove_stress_markers(phoneme):
    """Remove stress markers (digits) from phonemes"""
    return re.sub(r'[0-9]', '', phoneme)

def convert_words_to_phonemes(metadata_dir, splits, remove_stress=True, remove_punctuation=True):
    """
    Convert word transcripts to phoneme transcripts
    
    Args:
        metadata_dir (str): Directory containing .wrd files from LRS3 preprocessing
        splits (list): List of dataset splits to process
        remove_stress (bool): Whether to remove stress markers from phonemes
        remove_punctuation (bool): Whether to remove punctuation from phonemes
    
    Returns:
        dict: Phoneme counts for dictionary creation
    """
    print("üîÑ Initializing G2P converter...")
    g2p = G2p()
    
    # Set of all phonemes encountered
    phoneme_counts = Counter()
    punctuation = ["'", ",", "."]
    
    # Process each split
    for split in splits:
        input_file = Path(metadata_dir) / f"{split}.wrd"
        output_file = Path(metadata_dir) / f"{split}.phn"
        
        if not input_file.exists():
            print(f"‚ö†Ô∏è  Warning: {input_file} does not exist, skipping {split}")
            continue
            
        print(f"üìù Processing {split} split: {input_file}")
        total_sequences = 0
        total_phonemes = 0
        
        with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
            for line in tqdm.tqdm(f_in, desc=f"Converting {split}"):
                words = line.strip().split()
                if not words:  # Skip empty lines
                    f_out.write('\n')
                    continue
                    
                phonemes = []
                for word in words:
                    # Convert word to phonemes
                    word_phonemes = g2p(word)
                    
                    # Remove stress markers if requested
                    if remove_stress:
                        word_phonemes = [remove_stress_markers(p) for p in word_phonemes]
                    
                    # Filter out punctuation if requested
                    if remove_punctuation:
                        word_phonemes = [p for p in word_phonemes if p not in punctuation]
                    
                    # Update phoneme counts
                    phoneme_counts.update(word_phonemes)
                    phonemes.extend(word_phonemes)
                
                # Write phoneme sequence to output file
                f_out.write(' '.join(phonemes) + '\n')
                total_sequences += 1
                total_phonemes += len(phonemes)
        
        print(f"  ‚úÖ Created {output_file}")
        print(f"     üìä {total_sequences:,} sequences, {total_phonemes:,} phonemes")
    
    return phoneme_counts

def create_phoneme_dictionary(metadata_dir, phoneme_counts):
    """
    Create phoneme dictionary in the format expected by AV-HuBERT
    
    Args:
        metadata_dir (str): Directory to save the dictionary
        phoneme_counts (Counter): Phoneme frequency counts
    """
    # Calculate blank token count as 10% of total phoneme tokens
    total_phonemes = sum(phoneme_counts.values())
    blank_count = max(1, int(total_phonemes * 0.1))
    
    # Write dictionary in correct format: <token> <count>
    dict_file = Path(metadata_dir) / "dict.phn.txt"
    print(f"üìã Creating phoneme dictionary: {dict_file}")
    
    with open(dict_file, 'w') as f:
        # Add special blank token with calculated count
        f.write(f"<blank> {blank_count}\n")
        
        # Sort phonemes by count (descending) then alphabetically
        sorted_phonemes = sorted(phoneme_counts.items(), key=lambda x: (-x[1], x[0]))
        
        for phoneme, count in sorted_phonemes:
            if phoneme and phoneme != "<blank>":  # Skip empty and duplicate blank
                f.write(f"{phoneme} {count}\n")
    
    print(f"  ‚úÖ Dictionary created with {len(phoneme_counts)} unique phonemes")
    print(f"     üìä Total phoneme tokens: {total_phonemes:,}")
    print(f"     üìä Calculated <blank> count (10%): {blank_count:,}")
    
    # Show top 10 most frequent phonemes
    all_counts = [("<blank>", blank_count)] + sorted_phonemes
    all_sorted = sorted(all_counts, key=lambda x: (-x[1], x[0]))
    
    print("\nüìà Top 10 most frequent phonemes:")
    for i, (phoneme, count) in enumerate(all_sorted[:10], 1):
        print(f"  {i:2d}. {phoneme:<8} : {count:,}")
    
    return dict_file

def validate_files(metadata_dir, splits):
    """Validate that all required input files exist"""
    missing_files = []
    
    for split in splits:
        wrd_file = Path(metadata_dir) / f"{split}.wrd"
        if not wrd_file.exists():
            missing_files.append(str(wrd_file))
    
    if missing_files:
        print("‚ùå Error: Missing required .wrd files:")
        for file in missing_files:
            print(f"   {file}")
        print("\nüí° Make sure you've run the LRS3 preprocessing pipeline (step3_metadata_prep.py) first.")
        return False
    
    return True

def main():
    parser = argparse.ArgumentParser(
        description="Convert LRS3 word transcripts to phoneme transcripts with dictionary",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Basic usage with LRS3 metadata directory
    python create_phoneme_metadata.py --metadata-dir /path/to/lrs3/metadata
    
    # Process only train and test splits
    python create_phoneme_metadata.py --metadata-dir /path/to/lrs3/metadata --splits train,test
    
    # Keep stress markers and punctuation
    python create_phoneme_metadata.py --metadata-dir /path/to/lrs3/metadata --keep-stress --keep-punctuation

Output files:
    - train.phn, valid.phn, test.phn: Phoneme transcripts
    - dict.phn.txt: Phoneme dictionary with counts for training
        """
    )
    
    parser.add_argument(
        "--metadata-dir", 
        required=True, 
        help="Directory containing .wrd files from LRS3 step3_metadata_prep.py output"
    )
    parser.add_argument(
        "--splits", 
        default="train,valid,test", 
        help="Comma-separated list of dataset splits to process (default: train,valid,test)"
    )
    parser.add_argument(
        "--keep-stress", 
        action="store_true", 
        help="Keep stress markers (0,1,2) in phonemes (default: remove them)"
    )
    parser.add_argument(
        "--keep-punctuation", 
        action="store_true", 
        help="Keep punctuation symbols in phoneme set (default: remove them)"
    )
    
    args = parser.parse_args()
    
    # Validate arguments
    metadata_dir = Path(args.metadata_dir)
    if not metadata_dir.exists():
        print(f"‚ùå Error: Metadata directory does not exist: {metadata_dir}")
        return 1
    
    splits = [s.strip() for s in args.splits.split(',')]
    remove_stress = not args.keep_stress
    remove_punctuation = not args.keep_punctuation
    
    # Validate input files
    if not validate_files(metadata_dir, splits):
        return 1
    
    print("üöÄ Starting LRS3 phoneme metadata generation...")
    print(f"üìÅ Metadata directory: {metadata_dir}")
    print(f"üìã Processing splits: {', '.join(splits)}")
    print(f"üîß Remove stress markers: {remove_stress}")
    print(f"üîß Remove punctuation: {remove_punctuation}")
    print("-" * 60)
    
    try:
        # Convert words to phonemes
        phoneme_counts = convert_words_to_phonemes(
            metadata_dir, splits, remove_stress, remove_punctuation
        )
        
        if not phoneme_counts:
            print("‚ùå Error: No phonemes were generated. Check your input files.")
            return 1
        
        # Create phoneme dictionary
        dict_file = create_phoneme_dictionary(metadata_dir, phoneme_counts)
        
        print("-" * 60)
        print("üéâ Phoneme metadata generation completed successfully!")
        print(f"üìÅ Output directory: {metadata_dir}")
        print(f"üìã Generated files:")
        
        # List generated files
        for split in splits:
            phn_file = metadata_dir / f"{split}.phn"
            if phn_file.exists():
                print(f"   ‚Ä¢ {split}.phn")
        print(f"   ‚Ä¢ dict.phn.txt")
        
        print(f"\nüí° You can now use these phoneme files for AV-HuBERT training!")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Error during processing: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
